{
"question-1": "2011",
"question-2": "CM1, PC1, PC3, PC4 (from NASA)\nAR3, AR4, AR5, AR6 (from an industry partner)",
"question-3": "CM1: 498 modules\nPC1: 1109 modules\nPC3: 1563 modules\nPC4: 1458 modules\nAR3: 63 modules\nAR4: 107 modules\nAR5: 36 modules\nAR6: 101 modules",
"question-4": "Artificial Neural Networks (ANN)\nNaive Bayes (NB)\nVoting Feature Intervals (VFI)\nEnsemble of classifiers (Ens1 and Ens2)",
"question-5": "The study primarily focuses on comparing ensembles (Ens1, Ens2) with a benchmark study (Menzies et al. 2007a) and doesn't provide separate results (precision, recall, F1, AUC-ROC) for individual ANN, NB, and VFI on all datasets.",
"question-6": "The study doesn't explicitly list all the static code attributes used. It mentions using the top eleven attributes from the Promise repository based on Information Gain and prior research.",
"question-7": "The study doesn't provide analysis on the individual predictive power, ranking, or weighting of specific software metrics.",
"question-8": "Probability of Detection (pd) / Recall / Hit Rate\nProbability of False Alarms (pf)\nBalance (bal)\nPrecision (prec)",
"question-9": "Yes, Principal Component Analysis (PCA) was used for ANN to reduce dimensionality and complexity.",
"question-10": "Yes, two ensemble methods were used:\nEns1:  Combined ANN, NB, and VFI using majority voting.\nEns2: Combined NB and VFI using an AND logic for voting (predicting defective only if both agree).",
"question-11": "Yes, 10 * 10-fold stratified cross-validation was employed.",
"question-12": "This information is available in Table 2 of the paper, showing defect rates for each dataset.",
"question-13": "Log filtering: Attribute values were replaced with their logarithms.\nNormalization: Data was normalized to the range [0, 1] for ANN (after PCA).",
"question-14": "Yes, Information Gain was used to select the top eleven most relevant attributes.",
"question-15": "The study compares the proposed ensembles (Ens1, Ens2) with a benchmark study that used Naive Bayes (Menzies et al. 2007a).",
"question-16": "The paper doesn't specify the programming language or tools used for implementing the machine learning models.",
"question-17": "ANN: Not suitable for small datasets, requires larger datasets for parameter optimization.\nVFI: Comparable to NB, more robust but can have higher false alarms.\nNB: Relatively good at reducing false alarms, but might have lower detection rates compared to other techniques.\nEnsemble (Ens2):  Found to be beneficial for embedded software, balancing the trade-off between high detection rates and low false alarms.",
"question-18": "The paper doesn't explicitly mention whether the focus is within-project or cross-project defect prediction. However, the use of 10 * 10-fold cross-validation and the mention of \"local\" and \"external\" datasets suggest a focus on building models generalizable to unseen data, which aligns more with cross-project prediction.",
"question-19": "While not entirely novel, the study proposes a specific combination of techniques (NB and VFI with AND voting logic in Ens2) tailored for the context of embedded software and achieving a balance between detection rates and low false alarms.",
"question-20": "The study finds that no single machine learning technique consistently outperforms others.\nEnsemble methods, particularly Ens2, prove to be effective in leveraging the strengths of multiple classifiers and achieving a balance between detection rates and false alarms relevant to the industrial context.",
"question-21": "Data quality and noise are acknowledged as challenges in software defect prediction.\nThe study highlights the need for careful selection of algorithms and voting schemes within ensembles.\nLimited data availability for embedded systems is mentioned as a constraint.",
"question-22": "The paper doesn't discuss the computational complexity or runtime performance of the techniques used.",
"question-23": "Exploring the use of other algorithms in the ensemble.\nInvestigating weighted voting mechanisms for the ensemble.\nIncorporating requirement metrics as additional features in defect prediction models.\nReplicating the study in other embedded software development contexts.",
"question-24": "Transfer learning or domain adaptation is not explicitly mentioned in the study.",
"question-25": "The paper includes a ROC curve (Fig. 2).",
"question-26": "The paper doesn't discuss the interpretability of the models.",
"question-27": "Yes, the research was conducted in collaboration with an industry partner, and the developed model (Ens2) was integrated into their software development practices.",
"question-28": "Yes, the study performs a cost-benefit analysis to demonstrate the potential cost savings of using the proposed defect prediction model in reducing testing effort.",
"question-29": "The paper doesn't explicitly compare traditional software metrics with more advanced or domain-specific metrics. It focuses on using static code attributes commonly found in the Promise repository.",
"question-30": "The authors acknowledge the difficulty in claiming generalizability in software engineering research. They attempt to mitigate this by using datasets from different sources (NASA and an industry partner) and different application domains. However, they emphasize that the findings are primarily applicable to embedded software development contexts."
}