{
"question-1": "2018",
"question-2": "MC2, KC3, MW1, CM1, PC1, PC2, PC3, PC4, PC5, MC1, JM1, Bugzilla, Columba",
"question-3": "See Table 2 for the sizes of the MDP datasets. See Table 3 for the sizes of the JIT datasets.",
"question-4": "OneR, RIPPER (JRip), C4.5 (J48), NBTree, Ridge Logistic Regression (RLR), Support Vector Machines (SVM), Multilayer Perceptron (MLP), Gaussian Naive Bayes (NBc), Discrete Naive Bayes (NBd), Tree-augmented Naive Bayes (TAN), Averaged One-Dependence Estimators (AODE), Hidden Naive Bayes (HNB), AdaBoost (AdaBst), Random Forest (RF), Discrete Naive Bayes (NBd2), Naive Bayes Ensemble (NBE), Superposed Naive Bayes (SNB), Tree-augmented Naive Bayes (TAN2), Naive Bayes Ensemble + TAN (NBE2), Superposed Naive Bayes + TAN (SNB2)",
"question-5": "The paper primarily focuses on AUC-ROC values for evaluating accuracy. See Table 6 for the AUC-ROC values of all tested techniques. Precision, Recall, and F1-score are not reported.",
"question-6": "**MDP Datasets:** LOC counts (total, blank, code and comment, comments, executable), Halstead metrics (content, difficulty, effort, error est, length, level, prog_time, volume, num_operands, num_operators, num_unique_operands, num_unique_operators), McCabe metrics (cyclomatic_complexity, cyclomatic_density, design_complexity, essential_complexity), and miscellaneous metrics (branch count, call_pairs, condition count, decision_count, decision_density, design_density, edge_count, essential_density, global_data_complexity, global_data_density, maintenance_severity, modified_condition_count, multiple_condition_count, node count, normalized_cyclomatic_compl., parameter count, percent comments). (See Table 2)\n**JIT Datasets:** Diffusion metrics (NS, ND, NF, and Entropy), Size metrics (LAn, LDn, and LTn), Purpose metric (FIX), History metrics (NDEV, AGE, and NUCn), and Experience metrics (EXP, REXP, and SEXP). (See Table 3)",
"question-7": "The paper doesn't provide individual predictive power for each metric. Ranking: Importance rankings for the top metrics are provided for RLR, SNB, and RF+PDP models for both Bugzilla and Columba datasets. (See Table 9) Additional ranking information is available for the MDP datasets in Tables 10, 11, 12, 13, 14, and 15.",
"question-8": "Primarily Area Under the Receiver Operating Characteristic curve (AUC-ROC).",
"question-9": "No dimensionality reduction techniques were explicitly mentioned.",
"question-10": "AdaBoost, Random Forest, Proposed: Naive Bayes Ensemble (NBE),  Naive Bayes Ensemble + TAN (NBE2)",
"question-11": "5-fold cross-validation, repeated 5 times (25 total iterations) with different random data partitions.",
"question-12": "See Table 2 and Table 3 for the percentage of defective modules in each dataset.",
"question-13": "Cleaning: The NASA MDP datasets were used in their \"cleaned\" versions where problematic and irrelevant data were removed. Normalization:  In the JIT datasets, LAn, LDn, LTn, and NUCn were normalized to avoid high correlation among features. Discretization: Continuous variables were discretized for use with Naive Bayes models, using AIC or BIC as the criterion.",
"question-14": "No dedicated feature selection method is described. Implicit Selection: Discretization based on AIC or BIC allowed for concurrent calculation of independent variable importance, which could be used for variable selection.",
"question-15": "The accuracy evaluation approach was similar to that of Ghotra et al. (2015). The study revisited findings of Lessmann et al. (2008) and Ghotra et al. (2015) regarding the performance of different classification techniques. The SNB approach was inspired by the work of Ridgeway et al. (1998) on boosted Naive Bayes classifiers.",
"question-16": "Weka machine learning toolkit, Java, R",
"question-17": "Rule-based learners: High interpretability but potentially lower accuracy. Decision trees: Interpretable to a certain extent but can become complex and lose transparency with larger datasets. RLR: High interpretability and good accuracy but susceptible to multicollinearity. SVM and MLP:  High accuracy but black-box models with low interpretability. Naive Bayes: Interpretable with generally good performance. Ensemble learners: High accuracy but can be less interpretable than single models. SNB and SNB2: Aim for balanced accuracy and interpretability.",
"question-18": "The study focused on module-level defect prediction, but it doesn't specify whether it's within-project or cross-project. The use of publicly available datasets suggests that the evaluation might be leaning towards cross-project prediction.",
"question-19": "Superposed Naive Bayes (SNB):  Combines a Naive Bayes ensemble with a transformation back into a simple Naive Bayes model using linear approximation. Extended AdaBoost: Uses stochastic gradient descent and out-of-bag estimates to improve performance.",
"question-20": "The proposed SNB method was found to achieve a good balance of accuracy and interpretability. Ensemble methods like RF and AdaBoost generally showed the highest accuracy.",
"question-21": "The paper highlights the difficulty of objectively comparing interpretability across different classification models. The reliance on AUC-ROC as the sole performance measure might not capture the complete picture of model effectiveness.",
"question-22": "No explicit analysis of computational complexity or runtime performance was provided.",
"question-23": "Further investigation into the mechanisms through which SNB improves performance. Development of more objective and robust methods for measuring interpretability. Applying the proposed method to other domains where both accuracy and interpretability are important.",
"question-24": "Not considered in this study.",
"question-25": "Scatter plots were used to compare the RFRs of accuracy and interpretability. Weights of evidence (WoE) plots were used to illustrate the relationship between independent variables and their contribution to defect proneness in SNB models. Partial dependence plots (PDPs) were used to visualize variable importance and relationships in RF models.",
"question-26": "Interpretability is a key focus of the paper. Lipton's (2016) categories of interpretability (model transparency, component transparency, algorithmic transparency) were used for assessment.",
"question-27": "The paper uses datasets from NASA and open-source projects, indicating a practical relevance to software engineering. However, specific industrial applications or case studies were not discussed.",
"question-28": "Not discussed.",
"question-29": "No explicit comparisons between traditional and advanced/domain-specific metrics were made. However, the use of both code-level metrics (MDP) and change-level metrics (JIT) offers some insight into different types of metrics.",
"question-30": "The use of diverse datasets suggests an attempt to ensure generalizability. However, the authors acknowledge that the study's focus on a limited set of datasets and classification techniques could pose a threat to external validity. Further research with a broader scope is encouraged."
}