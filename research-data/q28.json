{
"question-1": "2010",
"question-2": "KC1 NASA dataset",
"question-3": "KC1: 145 classes (initially 669 faults, reduced to 642 after preprocessing)",
"question-4": "Support Vector Machine (SVM)",
"question-5": "Note: Multiple models were trained for different fault severity levels.\n    * **HSF (High Severity Fault) Model:**\n       * a. Accuracy: Not explicitly reported, derived as 70.09% ((86+16)/(102+48))\n       * b. Precision: 70.34% \n       * c. Recall (Sensitivity): 69.56% \n       * d. F1-score: Not reported\n       * e. AUC-ROC: 0.728\n    * **MSF (Medium Severity Fault) Model:**\n       * a. Accuracy: Not explicitly reported, derived as 83.37% ((77+47)/(87+58))\n       * b. Precision: 82.75% \n       * c. Recall (Sensitivity): 81.03% \n       * d. F1-score: Not reported\n       * e. AUC-ROC: 0.88\n    * **LSF (Low Severity Fault) Model:**\n       * a. Accuracy: Not explicitly reported, derived as 76.06% ((82+29)/(106+39))\n       * b. Precision: 77.08% \n       * c. Recall (Sensitivity): 74.35% \n       * d. F1-score: Not reported\n       * e. AUC-ROC: 0.84 \n    * **USF (Ungraded Severity Fault) Model:**\n       * a. Accuracy: Not explicitly reported, derived as 78.62% ((70+45)/(86+59))\n       * b. Precision: 78.62%\n       * c. Recall (Sensitivity): 76.27% \n       * d. F1-score: Not reported\n       * e. AUC-ROC: 0.89",
"question-6": "* Coupling Between Objects (CBO)\n* Lack of Cohesion (LCOM)\n* Number of Children (NOC)\n* Depth of Inheritance Tree (DIT)\n* Weighted Methods per Class (WMC)\n* Response for a Class (RFC)\n* Source Lines of Code (SLOC)",
"question-7": "* a. Individual predictive power was not explicitly quantified for each metric. However, sensitivity, specificity, precision, and completeness values are presented for each metric individually and in the combined model.\n* b.  Metrics were not explicitly ranked or weighted.",
"question-8": "* Sensitivity \n* Specificity \n* Precision \n* Completeness\n* Area Under the ROC Curve (AUC-ROC)",
"question-9": "No dimensionality reduction techniques were explicitly mentioned.",
"question-10": "No ensemble methods were used.",
"question-11": "Yes, 10-fold cross-validation was used.",
"question-12": "KC1: 59 defective classes out of 145 (approximately 40.69% defective)",
"question-13": " Faults that were not bugs were removed (reducing faults from 669 to 642).",
"question-14": "No formal feature selection method was applied. All seven metrics were used in the models.",
"question-15": "Yes, the study compared its results with several other studies that used the same dataset and other techniques like logistic regression, decision trees, and artificial neural networks.",
"question-16": "The paper does not explicitly mention the programming language or tool used for implementing SVM.",
"question-17": "Specific strengths or weaknesses of SVM were not explicitly discussed. However, the paper mentions that SVM performed better than logistic regression and comparably to decision trees for this dataset.",
"question-18": "The study focuses on within-project defect prediction as it utilizes a single dataset (KC1).",
"question-19": "No novel or hybrid techniques were proposed in this study.",
"question-20": "SVM showed promising results, outperforming logistic regression and achieving comparable performance to decision trees for this particular dataset.",
"question-21": "* The study acknowledges the need for external validation with different datasets and programming languages to generalize the findings.\n* Subjectivity of fault severity rating in the dataset was mentioned as a potential limitation.",
"question-22": "No analysis of computational complexity or runtime performance was provided. ",
"question-23": "* Replicating the study with different datasets and programming languages. \n* Exploring other machine learning algorithms like genetic algorithms.\n* Conducting cost-benefit analyses of fault prediction models.",
"question-24": "Not considered in this study.",
"question-25": "Yes, bar graphs for fault distribution and line graphs to illustrate sensitivity and completeness at various cutoff points were provided.",
"question-26": "No specific discussion on model interpretability was included.",
"question-27": "The paper discusses the potential application of the models in planning and resource allocation for testing activities.",
"question-28": "Mentioned as a potential future research direction but not analyzed in this study.",
"question-29": "The study focused only on traditional object-oriented metrics. No comparisons were made with more advanced or domain-specific metrics.",
"question-30": "The authors acknowledge that the generalizability of their findings is limited due to the use of a single dataset. They recommend further validation with diverse datasets and programming languages."
}