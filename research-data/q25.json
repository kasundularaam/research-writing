{
"question-1": "2018",
"question-2": "10 projects from the PROMISE repository: Ant1, Ant2, Camel1, Camel2, Forrest, Ivy, Jedit, Synapse, Xalan, Xerces. \n10 manually extracted open source projects: Clam Antivirus, eCos, Helma, NetBSD, OpenBSD, OpenCMS, openNMS, Scilab, Spring Security, XORP.",
"question-3": "The number of instances varies per project and version. It ranges from 29 instances (Forrest) to 10960 instances (NetBSD version B). Refer to Table 2 in the paper for details.",
"question-4": "Random Forest (RF)\nC4.5 Decision Tree (C4.5)\nSupport Vector Machines (SVM)\nNeural Networks (NNET) - 3-layer model\nK-Nearest Neighbor (KNN)",
"question-5": "This paper focuses on comparing performance when applying sampling methods, not individual performance. They did not report specific values for accuracy, precision, recall, F1-score for each technique individually. However, they use these metrics comparatively.",
"question-5e": "AUC-ROC was one of the primary performance measures studied. The paper concludes that resampling techniques did not significantly improve AUC values.",
"question-6": "20 static code metrics (listed in Table 1) including WMC, DIT, NOC, etc. \n8 process metrics (listed in Table 1) including CodeChurn, LOCAdded, LOCDeleted, etc.",
"question-7": "The study doesn't focus on the individual predictive power of each metric. The emphasis is on the impact of resampling techniques on the overall model performance.",
"question-8": "AUC (Area under the ROC curve)\ng-mean (Geometric mean)\nbalance\npd (Recall)\npf (Probability of false alarms)",
"question-9": "No dimensionality reduction techniques were explicitly mentioned.",
"question-10": "The paper mentions prior studies using ensemble methods but this study doesn't employ them directly. The focus is on the impact of resampling techniques.",
"question-11": "10-fold cross-validation was used during model construction for parameter optimization.",
"question-12": "The imbalance ratio (Pfp) ranged from 3.8% to 17.46%. This means defective modules were significantly fewer than non-defective modules in each dataset.",
"question-13": "Min-max normalization was applied to the training and testing datasets.\nResampling techniques (SMOTE, Borderline-SMOTE, Safe-level SMOTE, ADASYN, Random Over Sampling, Random Under Sampling) were applied to the training data at various percentage of fault-prone modules (Pfp) levels.",
"question-14": "The paper doesn't mention any specific feature selection techniques.",
"question-15": "Yes, the paper compares its findings to several prior studies on resampling techniques in software defect prediction. The related work section (Section 2) provides a detailed overview.",
"question-16": "The models were implemented and evaluated using the R programming language with the Caret library package. MATLAB was used for implementing some of the resampling techniques.",
"question-17": "RUS: Simple, performs well for defect classification but can lead to high false alarms.\nBorderline-SMOTE: More stable than other oversampling methods, shows good performance but also increases false alarms.\nSMOTE and variants:  Can improve performance but less stable than Borderline-SMOTE, can significantly increase false alarms.\nADASYN:  Generally performed poorly, led to high false alarms.\nC4.5:  Showed resistance to changes due to sampling compared to other prediction models.",
"question-18": "Within-project defect prediction. Models were trained on a previous version of a project and tested on a new version.",
"question-19": "No novel techniques were proposed. The focus is on the evaluation of existing techniques.",
"question-20": "Resampling is beneficial for defect classification (finding all defective modules) but not for defect prioritization (ranking defective modules).\nRUS and Borderline-SMOTE were the most effective methods overall.\nThe choice of resampling method and Pfp depends on the performance measure used and the specific project characteristics.",
"question-21": "High false alarm rates associated with most resampling methods.\nDifficulty in finding a stable and universally optimal Pfp rate.\nThe paper highlights the need for newer resampling techniques that can address noise and outliers in defect datasets.",
"question-22": "No specific analysis of runtime performance or complexity was presented.",
"question-23": "Develop new resampling techniques that generate relevant and informative data samples.\nDevelop techniques that address noise and outliers in defect datasets.\nConduct studies on configuring parameters of sampling methods (especially SMOTE) to further improve performance.",
"question-24": "No, transfer learning was not considered in this study.",
"question-25": "Yes, quartile charts and dot plots were used to visualize the performance results of different resampling techniques at various Pfp rates.",
"question-26": "The paper didn't focus on model interpretability. The primary aim was to compare the performance of different resampling methods.",
"question-27": "No specific real-world applications were discussed. The paper primarily focuses on research within the software engineering domain.",
"question-28": "The paper touches upon the cost associated with false alarms (testing non-defective modules) but doesn't provide a detailed cost-effectiveness analysis.",
"question-29": "No comparisons were made between different types of metrics. The study used a combination of established static code and process metrics.",
"question-30": "The authors acknowledge that using open source projects limits generalizability to commercial software. Further research with commercial datasets is needed."
}