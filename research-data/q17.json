{
"question-1": "April 2019",
"question-2": "mozilla4, kc1, ar1, ar4, pc1",
"question-3": "mozilla4: 15544 modules\nkc1: 2108 modules\nar1: 121 modules\nar4: 121 modules\npc1: 1107 modules",
"question-4": "k-Nearest Neighbors (KNN)\nDecision Tree (DT)\nNaive Bayes (NB)\nSupport Vector Machine (SVM)\nArtificial Neural Network (ANN)",
"question-5": "Accuracy, Precision, Recall, F1-score, AUC-ROC (not explicitly reported)",
"question-6": "McCabe, Halstead, Line Count, Operator, Branch Count",
"question-7": "Not explicitly analyzed in this study.",
"question-8": "Accuracy, Precision, Recall, F1-measure",
"question-9": "Yes, Principal Component Analysis (PCA)",
"question-10": "Yes, Bagging and Voting",
"question-11": "Yes, 10-fold cross-validation",
"question-12": "mozilla4: Not precisely given, but implies a small percentage of defective modules.\nkc1:  325 defective modules out of 2108 (roughly 15.4%)\nar1 & ar4: 9 defective modules out of 121 (roughly 7.4%)\npc1: 76 defective modules out of 1107 (roughly 6.9%)",
"question-13": "Yes, PCA was used for dimensionality reduction.",
"question-14": "While mentioned as important, the specific method used wasn't explicitly stated. It seems likely that feature selection was implicitly done through the use of established software metrics like McCabe and Halstead.",
"question-15": "Yes, the study referred to and compared its results with several other studies in the literature (see Related Work section).",
"question-16": "WEKA (Waikato Environment for Knowledge Analysis)",
"question-17": "The study generally highlighted that different classifiers have varying strengths and weaknesses depending on the dataset and parameter tuning.\nFor example, ANN and NB performed well on the ar1 dataset, while KNN showed good results on arl, kc1, and pc1 datasets.",
"question-18": "The study primarily focused on within-project defect prediction, aiming to identify defects within the same software project.",
"question-19": "Yes, the use of Ensemble Learning (Bagging and Voting) with combined classifiers (like Decision Tree and Support Vector Machine) was a key contribution.",
"question-20": "Ensemble learning methods generally outperformed individual classifiers.\nParameter tuning was crucial for improving accuracy.\nPCA didn't show a significant impact on performance in this study.",
"question-21": "Not explicitly discussed in detail.",
"question-22": "Not analyzed in this study.",
"question-23": "Explore more sophisticated feature selection methods.\nUtilize larger datasets.\nApply other machine learning algorithms.",
"question-24": "Not considered in this study.",
"question-25": "Yes, bar charts (Figures 2-7) were provided to compare the performance of different classifiers and settings.",
"question-26": "Not discussed.",
"question-27": "Not explicitly discussed.",
"question-28": "Not analyzed.",
"question-29": "Not explicitly compared.",
"question-30": "Not extensively discussed, but the use of multiple datasets suggests an attempt to assess generalizability to some extent."
}