{
"question-1": "2018 (From the conference proceedings title)",
"question-2": "* JM1\n* KC1\n* PC1\n* jEdit 4.2-4.3",
"question-3": "* JM1: 10885 instances\n* KC1: 2109 instances\n* PC1: 1109 instances\n* jEdit 4.2-4.3: 369 instances",
"question-4": "* Decision Tree (DT)\n* Random Forest (RF)\n* Naïve Bayes (NB)\n* Support Vector Machine (SVM)\n* Artificial Neural Network (ANN)\n* Adaboost",
"question-5": "This information is best extracted directly from Table 2 in the paper, as the values vary across datasets and with/without feature selection. Here's a general summary:\n\n* **Random Forest** generally had the highest accuracy, F1-score, and AUC-ROC values across most datasets and feature selection scenarios.\n* **ANN** often performed well, sometimes second to Random Forest.\n* **SVM** had highly variable performance, struggling particularly when dimensionality reduction was applied.\n* **Naïve Bayes** consistently had the lowest accuracy.\n* The paper doesn't explicitly list AUC-ROC for all individual classifier/dataset combinations.",
"question-6": "The paper mentions these types of metrics were used:\n\n* Chidamber and Kemerer (CK) metrics\n* McCabe metrics\n* HalStead metrics\n* It also mentions LOC (Lines of Code) and Miscellaneous metrics for jEdit\n\nThe exact metrics within these categories aren't explicitly listed in the paper.",
"question-7": "The paper doesn't report the individual predictive power or ranking/weighting of each specific metric. This level of detail is often omitted in such studies.",
"question-8": "* Accuracy\n* F-measure (F1-score)\n* Precision\n* Recall\n* Area Under ROC (Receiver Operating Characteristic) Curve (AUC)",
"question-9": "Yes, Principal Component Analysis (PCA) was used.",
"question-10": "Yes,\n* Random Forest (an ensemble of decision trees)\n* Adaboost (another ensemble method)",
"question-11": "Yes, 10-fold cross-validation was used.",
"question-12": "This is given in Table 1 as \"%Faulty\":\n\n* JM1: 19.35%\n* KC1: 15.46%\n* PC1: 6.94%\n* jEdit 4.2-4.3: 44.72%",
"question-13": "The paper doesn't explicitly mention specific data preprocessing steps beyond feature selection.",
"question-14": "Yes, two feature selection methods were used with different search strategies:\n\n* **Correlation-based Feature Subset Selection (CFS)** with **GreedyStepwise** search\n* **Principal Component Analysis (PCA)** with **Ranker** search method",
"question-15": "Yes, the \"Related Works\" section (Section II) discusses several other studies on software fault prediction using various techniques, providing context for their work.",
"question-16": "The models were implemented using WEKA (Waikato Environment for Knowledge Analysis) software, which is written in Java.",
"question-17": "* **Strength of Random Forest:**  High accuracy and good performance across datasets, particularly with CFS feature selection.\n* **Weakness of SVM:**  Sensitivity to dimensionality reduction, leading to performance drops with PCA.\n* **Weakness of Naïve Bayes:** Consistently low accuracy compared to other techniques.",
"question-18": "The paper doesn't explicitly state this, but based on the use of single datasets for each experiment, it appears to be focused on **within-project** defect prediction.",
"question-19": "No, the study primarily focuses on evaluating the performance of existing, well-known machine learning techniques.",
"question-20": "* Random Forest was the most effective in most cases.\n* Feature selection using CFS was generally beneficial.\n* PCA was not as effective for feature selection in this context.\n* The study supports the use of machine learning for software fault prediction.",
"question-21": "No specific challenges or limitations were explicitly mentioned in the conclusion.",
"question-22": "No, the paper doesn't include analysis of computational complexity or runtime.",
"question-23": "Yes, they recommend further studies with larger-scale software projects to validate the generalizability of the results.",
"question-24": "No, transfer learning or domain adaptation was not mentioned.",
"question-25": "Yes, ROC curves were provided for each classifier and dataset, with and without feature selection. Bar charts comparing AUC values were also included.",
"question-26": "No, the paper doesn't discuss the interpretability of the models.",
"question-27": "The datasets used are from real-world projects (NASA, etc.), but the paper doesn't discuss specific industrial applications beyond the research context.",
"question-28": "No, cost-effectiveness was not analyzed.",
"question-29": "No explicit comparison was made, though the study uses commonly employed traditional metrics.",
"question-30": "They mention the need for further studies with larger projects to assess generalizability, implicitly acknowledging that the current results may have limitations in their generalizability."
}