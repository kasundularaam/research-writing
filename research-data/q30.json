{
"question-1": "2010",
"question-2": "KC1 from the NASA Metrics Data Program (MDP)",
"question-3": "145 classes",
"question-4": "Decision Tree (DT)\nArtificial Neural Network (ANN)",
"question-5": "  | Metric | Model | Severity | Sensitivity | Specificity | Precision | Completeness | AUC-ROC |\n  |---|---|---|---|---|---|---|---||\n  | **DT** | Model I | High | 69.50 | 90.00 | 84.68 | 70.80 | 0.766 |\n  | **DT** | Model II | Medium | 89.70 | 71.30 | 78.64 | 97.50 | 0.888 |\n  | **DT** | Model III | Low | 90.30 | 81.10 | 83.40 | 94.30 | 0.875 |\n  | **DT** | Model IV | Ungraded | 91.50 | 87.20 | 88.90 | 94.00 | 0.835 |\n  | **ANN** | Model I | High | 73.90 | 71.31 | 71.70 | 87.50 | 0.722 |\n  | **ANN** | Model II | Medium | 74.10 | 74.00 | 73.79 | 85.07 | 0.809 |\n  | **ANN** | Model III | Low | 71.70 | 72.64 | 72.41 | 79.31 | 0.765 |\n  | **ANN** | Model IV | Ungraded | 72.80 | 79.06 | 76.55 | 88.16 | 0.809 |\n\n  **Note:** The F1-score is not explicitly reported in the paper.",
"question-6": "* Coupling Between Objects (CBO)\n* Weighted Methods per Class (WMC)\n* Response for a Class (RFC)\n* Source Lines of Code (SLOC)\n* Lack of Cohesion (LCOM)\n* Number of Children (NOC)\n* Depth of Inheritance (DIT)",
"question-7": "* The paper analyzes the individual predictive power through univariate logistic regression and presents R² values for each metric across different severity levels (See Tables 5, 6, 7, 8).\n* While not explicitly ranked, the study identifies SLOC and CBO as the strongest predictors based on their consistently high R² values.\n* The study doesn't explicitly assign weights to the metrics.",
"question-8": "* Sensitivity\n* Specificity\n* Precision\n* Completeness\n* Area Under the ROC Curve (AUC-ROC)",
"question-9": "No dimensionality reduction techniques were used.",
"question-10": "No ensemble methods were used.",
"question-11": "Yes, 10-fold cross-validation was used.",
"question-12": "Out of 145 classes, 59 were defective (faulty), and the rest were non-defective.",
"question-13": "* Removed faults classified as \"not a fault\".\n* Min-max normalization was applied to normalize input metric values to the range [0, 1].",
"question-14": "Forward stepwise selection was used for logistic regression. All features selected in univariate analysis were included for the backward elimination method in ANN.",
"question-15": "Yes, the study compared its findings to numerous previous studies on object-oriented metrics and fault proneness prediction. These comparisons are summarized in Table 24 and discussed throughout the paper.",
"question-16": "The paper does not specify the programming language or tool used for implementing the machine learning models.",
"question-17": "* **DT:** Showed the best overall performance in terms of AUC-ROC and completeness across different severity levels.\n* **ANN:** Performed slightly worse than DT but still better than LR.\n* **LR:** Simplest method, but its performance was lower than both DT and ANN.",
"question-18": "The study focuses on within-project defect prediction, utilizing data from a single NASA project (KC1).",
"question-19": "No novel or hybrid techniques were proposed. The study focused on evaluating the performance of established machine learning methods.",
"question-20": "The study concludes that DT and ANN models outperform LR models for fault proneness prediction across all severity levels.",
"question-21": "* The study acknowledges the general limitations of empirical studies, including potential generalizability issues due to using data from a single project.\n* Subjectivity and potential inaccuracies in fault severity ratings within the KC1 dataset are also mentioned as limitations.",
"question-22": "The paper does not discuss the computational complexity or runtime performance of the different techniques.",
"question-23": "* Replicate the study with different datasets to generalize the findings.\n* Explore the application of other machine learning algorithms, such as genetic algorithms.\n* Conduct cost-benefit analyses of different fault proneness prediction models.",
"question-24": "Transfer learning or domain adaptation was not considered in this study.",
"question-25": "Yes, ROC curves were used to visualize and compare the performance of different models (Figures 2, 3, 4).",
"question-26": "The paper doesn't focus on interpreting the models. The analysis primarily relies on performance metrics for comparison.",
"question-27": "The paper mentions the potential applicability of the models for software practitioners in planning and prioritizing testing efforts. However, it doesn't discuss specific real-world case studies.",
"question-28": "The cost-effectiveness of the bug prediction approach is mentioned as a potential future research direction but is not explicitly analyzed in the study.",
"question-29": "No comparisons were made between traditional and advanced software metrics.",
"question-30": "The paper acknowledges the limited generalizability of the results due to using data from a single project and recommends replicating the study with different datasets to draw stronger conclusions."
}