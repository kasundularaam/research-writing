{
"question-1": "2007",
"question-2": "Eclipse bug dataset (releases 2.0, 2.1, and 3.0)",
"question-3": "Release 2.0: 6,740 files and 376 packages\nRelease 2.1: 7,900 files and 433 packages\nRelease 3.0: 6,614 files and 429 packages",
"question-4": "Logistic Regression (for classification)\nLinear Regression (for ranking)",
"question-5": "Logistic Regression:\nAccuracy: Ranges from 0.682 to 0.789 for files, 0.612 to 0.757 for packages.\nPrecision: Ranges from 0.578 to 0.687 for files, 0.641 to 0.785 for packages.\nRecall: Ranges from 0.185 to 0.379 for files, 0.617 to 0.789 for packages.\nF1-score: Not reported.\nAUC-ROC: Not reported.\nLinear Regression:\nAccuracy: Not applicable (regression task).\nPrecision: Not applicable (regression task).\nRecall: Not applicable (regression task).\nF1-score: Not applicable (regression task).\nAUC-ROC: Not applicable (regression task).\nSpearman Correlation: Ranges from 0.331 to 0.640 for files, 0.368 to 0.901 for packages.",
"question-6": "FOUT (Number of method calls)\nMLOC (Method lines of code)\nNBD (Nested block depth)\nPAR (Number of parameters)\nVG (McCabe cyclomatic complexity)\nNOF (Number of fields)\nNOM (Number of methods)\nNSF (Number of static fields)\nNSM (Number of static methods)\nACD (Number of anonymous type declarations)\nNOI (Number of interfaces)\nNOT (Number of classes)\nTLOC (Total lines of code)\nNOCU (Number of files/compilation units)",
"question-7": "Individual predictive power of most metrics is analyzed through Spearman correlation with defect counts (Table 3).\nNo specific ranking or weighting of metrics is applied. All metrics are used as features in the models.",
"question-8": "Precision\nRecall\nAccuracy\nSpearman Correlation\nR-squared (RÂ²)",
"question-9": "No",
"question-10": "No",
"question-11": "Yes, across different releases of Eclipse (training on one release and testing on another).",
"question-12": "Not explicitly mentioned in the paper, but Figure 3 suggests that a large percentage of packages have no observed defects.",
"question-13": "No specific mention, but aggregation of class and method level metrics to file and package levels is performed.",
"question-14": "No",
"question-15": "Mentions other studies that used complexity metrics or historical data to predict failures (Section 2).",
"question-16": "Not mentioned.",
"question-17": "Not explicitly discussed.",
"question-18": "Cross-project (models are trained on one release of Eclipse and tested on another).",
"question-19": "No",
"question-20": "Suggests that combining complexity metrics can be useful for defect prediction but acknowledges limitations in prediction accuracy.",
"question-21": "Achieving high prediction accuracy.\nGeneralizability of models across projects and over time.",
"question-22": "Not discussed.",
"question-23": "Investigating better indicators for defects beyond complexity metrics.\nExploring applicability of models across projects and over time.\nIntegrating prediction models into the development process.",
"question-24": "Not considered.",
"question-25": "Yes, a histogram of post-release defects for packages (Figure 3).",
"question-26": "Not discussed.",
"question-27": "Not discussed.",
"question-28": "Not discussed.",
"question-29": "Not explicitly discussed.",
"question-30": "Acknowledges the need for further research to assess generalizability across projects and domains."
}