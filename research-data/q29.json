{
"question-1": "2023",
"question-2": "PROMISE dataset\nNASA dataset",
"question-3": "PROMISE: 2775 instances\nNASA: 11262 instances",
"question-4": "Adaptive Artificial Jelly Optimization (A2JO) - for feature selection \nLong Short-Term Memory (LSTM) - for bug prediction (classification)",
"question-5": "Accuracy:\nPROMISE: Varies by project, highest is 93.41% (Prop 4)\nNASA: Varies by project, highest is 92.8% (PC1)\nPrecision: Not reported individually for each project\nRecall: Not reported individually for each project\nF1-score:\nPROMISE: Varies by project, highest is 0.883 (Prop 4)\nNASA: Varies by project, highest is 0.962 (PC1)\nAUC-ROC: Not reported",
"question-6": "PROMISE: Wmc, Dit, Noc, Cbo, Rfc, Lcom, Ca, Ce, Npm, lcom3, Loc, Dam, Moa, Mfa, Cam, Ic, Cbm, Amc, Max_cc, Avg_cc  (See Table 1 for descriptions)\nNASA: Line count of code, Count of blank lines, Count of code and comments, Count of comments, Line count of executable code, Number of operators, Number of operands, Number of unique operators, Number of unique operands, Halstead metrics (Length, Volume, Level, Difficulty, Content, Effort, Error Estimate, Programming_Time), Cyclomatic_Complexity, Design_Complexity, Essential_Complexity. (See Table 2)",
"question-7": "Not reported in detail for each metric.",
"question-8": "Accuracy\nF-measure\nG-measure\nMatthews Correlation Coefficient (MCC)",
"question-9": "Not explicitly mentioned, likely not used.",
"question-10": "Not used.",
"question-11": "Not mentioned, likely not used.",
"question-12": "Not explicitly mentioned.",
"question-13": "Removal of duplicate instances",
"question-14": "Yes, using the A²JO algorithm.",
"question-15": "Yes, with several other studies and techniques:\nANN \nKNN \nNaive Bayes (NB)\nRandom Forest (RF)\nSupport Vector Machine (SVM)\nSpecific studies referenced in the paper: [14], [16], [17], [18], [19], [20]",
"question-16": "Python (mentioned in the Experimental Setup)",
"question-17": "Not discussed in detail for each individual technique.",
"question-18": "The paper doesn't explicitly specify. It implies a focus on within-project prediction by using different projects as separate datasets.",
"question-19": "Yes, the paper proposes:\nA²JO (Adaptive Artificial Jelly Optimization): A combination of the traditional AJO algorithm with Chaotic Opposition Based Learning (COBL) for enhanced feature selection.\nCombination of A²JO and LSTM: Using the selected features from A²JO as input to the LSTM model for improved bug prediction.",
"question-20": "The combination of A²JO and LSTM outperforms other standard machine learning techniques (ANN, KNN, NB, RF, SVM) in terms of accuracy, F-measure, G-measure, and MCC.\nThe proposed approach shows promising results for software bug prediction.",
"question-21": "Not explicitly discussed in a dedicated section.",
"question-22": "Not analyzed in the paper.",
"question-23": "Explore hybrid deep learning techniques for potential further improvement.\nAddress the class imbalance problem (if it exists in the datasets)\nInvestigate the use of optimization, vectorization, and broadcast techniques for faster and more efficient implementation.\nExperiment with different deep learning frameworks.",
"question-24": "Not considered in this study.",
"question-25": "Yes, bar charts are used to compare the proposed approach with other techniques.",
"question-26": "Not discussed.",
"question-27": "Not explicitly discussed.",
"question-28": "Not analyzed.",
"question-29": "No comparison made.",
"question-30": "Not directly addressed, but the use of multiple datasets suggests some level of generalizability. However, further research is needed to confirm broader applicability."
}