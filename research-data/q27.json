{
"question-1": "2014",
"question-2": "Ant (version 1.7)\nTomcat (version 6.0)\nJedit (version 4.3)\nVelocity (version 1.6)\nSynapse (version 1.2)\nPoi (version 3)\nLucene (version 2.4)\nXalan (version 2.5)\nIvy (version 2.0)",
"question-3": "Ant: 745 instances\nTomcat: 858 instances\nJedit: 492 instances\nVelocity: 229 instances\nSynapse: 256 instances\nPoi: 442 instances\nLucene: 340 instances\nXalan: 741 instances\nIvy: 352 instances",
"question-4": "Bayesian Networks",
"question-5": "a. Overall Accuracy: Not explicitly reported.\nb. Precision: Not explicitly reported.\nc. Recall: Not explicitly reported.\nd. F1-score: Not explicitly reported.\ne. AUC-ROC: Reported for each dataset (see Table 3 in the paper):\n   - Ant: 0.820\n   - Tomcat: 0.766\n   - Poi: 0.845\n   - Jedit: 0.658\n   - Velocity: 0.678\n   - Synapse: 0.660\n   - Lucene: 0.633\n   - Xalan: 0.624\n   - Ivy: 0.846",
"question-6": "LOC (Lines of Code)\nCBO (Coupling Between Objects)\nWMC (Weighted Methods per Class)\nRFC (Response for Class)\nLCOM (Lack of Cohesion of Methods)\nLCOM3 (Lack of Cohesion in Methods - Henderson-Sellers definition)\nDIT (Depth of Inheritance Tree)\nNOC (Number of Children)\nLOCQ (Lack of Coding Quality) - *Novel metric introduced in this paper*\nNOD (Number of Developers) - *Extracted for a subset of datasets*",
"question-7": "The paper primarily focuses on ranking metrics based on their influence on defect proneness within Bayesian Networks rather than individual predictive power.\nThe study uses a combination of expert knowledge and Bayesian Network structure to derive importance (see Section 4.2).\nGeneral Ranking:\n   - Most Effective: LOC, CBO, LOCQ, RFC, WMC \n   - Less Effective:  LCOM, LCOM3\n   - Least Effective: DIT, NOC",
"question-8": "AUC-ROC (Area Under the ROC Curve) is the primary measure.",
"question-9": "No, PCA or similar techniques were not used.",
"question-10": "No, ensemble methods were not used in this study.",
"question-11": "Yes, 10-fold cross-validation was used, and the experiments were repeated on 20 different 2/3rd subsets of each dataset to mitigate conclusion instability (see Section 5.3).",
"question-12": "This ratio varies for each dataset and is expressed as the \"% defective instances\" in Table 2 of the paper.",
"question-13": "Stratified sampling was used when creating subsets for cross-validation.\nDiscretization was applied to defect proneness, creating three states: defect-free, less defective, and more defective (for analysis with NOD).",
"question-14": "Yes, two methods were used for comparison:\n   - CFS (Correlation-based Feature Selection)\n   - ReliefF (Relief Feature Selection)",
"question-15": "The paper compares the results of Bayesian Networks to feature selection methods (CFS and ReliefF) to validate findings.\nIt discusses findings in the context of related work on bug prediction using different metrics and techniques.",
"question-16": "Weka (Java-based machine learning toolkit) was used for Bayesian Network implementation.",
"question-17": "Bayesian Networks:\n   - Strength: Can model complex relationships between metrics and handle uncertainty. \n   - Weakness:  Structure learning can be computationally expensive, and interpretability of large networks can be challenging.",
"question-18": "The paper does not explicitly state the focus but implies a within-project prediction scenario, as it analyzes metrics at the class level within individual projects (datasets).",
"question-19": "The introduction of the LOCQ (Lack of Coding Quality) metric, derived from static analysis, is a novel contribution.\nThe study of the impact of the number of developers (NOD) on defect proneness adds a new dimension to traditional metric-based prediction.",
"question-20": "The study found that Bayesian Networks, using the chosen software metrics, can be effective in predicting defect proneness.",
"question-21": "Conclusion instability: The paper acknowledges the problem of varying results across datasets and subsets and uses techniques to address it.\nNeed for more data and metrics:  The authors suggest that including additional software process metrics and expanding to more datasets would strengthen the research.",
"question-22": "The paper doesn't provide a dedicated analysis of computational complexity.",
"question-23": "Include more software process metrics.\nExperiment with a larger number of datasets.\nInvestigate other search algorithms for Bayesian Network structure learning.",
"question-24": "Not considered in this study.",
"question-25": "The paper provides visualizations of the learned Bayesian Networks for several datasets (Figures 4, 5, 6, 7, 8).\nIt includes a graph comparing the non-defectiveness probability for different numbers of developers (Figure 9).",
"question-26": "The paper partially addresses interpretability by analyzing the relationships between metrics in the learned Bayesian Networks. However, it does not use specific techniques to enhance model interpretability.",
"question-27": "Not explicitly discussed. The paper focuses on empirical evaluation using open-source datasets.",
"question-28": "Not analyzed in this study.",
"question-29": "The study primarily focuses on traditional software metrics. LOCQ, derived from static analysis, can be considered a more advanced metric, but the paper does not explicitly compare its effectiveness to traditional metrics.",
"question-30": "The authors acknowledge that the generalizability of their findings is limited by the number and type of datasets used. They encourage further research with more diverse projects and domains."
}