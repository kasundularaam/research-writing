{
    "question-1": "2019",
    "question-2": "27 datasets were used.  They are listed in Table 8, but are derived from these 13 projects:\n* ant (multiple versions)\n* camel (multiple versions)\n* ivy-2.0\n* jedit (multiple versions)\n* log4j-1.0\n* poi-2.0\n* synapse (multiple versions)\n* velocity-1.6\n* xerces (multiple versions)\n* Eclipse JDT Core\n* Eclipse PDE UI\n* Equinox Framework\n* Lucene\n* Mylyn",
    "question-3": "Sizes (number of modules/instances) are provided in Table 8. They range from 125 modules to 1862 modules.",
    "question-4": "* **Traditional Learners (Base Classifiers):**\n    * C4.5 (Decision Tree)\n    * RF (Random Forest)\n    * SVM (Support Vector Machine, using SMO implementation)\n    * Ripper (Rule-based)\n    * IBk (Instance-based k-Nearest Neighbor)\n    * LR (Logistic Regression)\n    * NB (Naïve Bayes)\n* **Imbalanced Learning Methods:** \n    * Bag (Bagging)\n    * Bst (Boosting) \n    * US (Under-Sampling)\n    * OS (Over-Sampling)\n    * UOS (UnderOver-Sampling)\n    * SMOTE (Synthetic Minority Oversampling Technique)\n    * COS (Cost-Sensitive Learning)\n    * EM1v1 (Ensemble method with splitting and coding techniques)\n    * UBag (UnderBagging)\n    * OBag (OverBagging)\n    * UOBag (UnderOverBagging)\n    * SBag (SMOTEBagging)\n    * UBst (UnderBoosting)\n    * OBst (OverBoosting)\n    * UOBst (UnderoverBoosting)\n    * SBst (SMOTEBoosting)\n    * Plus a \"Null\" imbalanced method (doing nothing) as a benchmark",
    "question-5": "The paper does *not* report individual accuracy, precision, recall, F1, or AUC for each combination of learner and imbalanced method. It focuses on the *relative difference* in performance between using an imbalanced learning technique vs. not using one.",
    "question-6": "* **Source Code Metrics (CK):** The Chidamber-Kemerer metrics suite (6 metrics) plus LOC (Lines of Code). Details in Appendix A.1\n* **Network Metrics (NET):** Social Network Analysis (SNA) metrics based on software dependency graphs (25 metrics). Details in Appendix A.2.\n* **Process Metrics (PROC):** 11 metrics related to the development process (e.g., number of revisions, authors). Details in Appendix A.3.\n* **Combinations:** The study also explored combinations of the above metric classes: CK+NET, CK+PROC, NET+PROC, CK+NET+PROC",
    "question-7": "The paper does not report on the individual predictive power of each metric. It focuses on the overall impact of using different classes of metrics.",
    "question-8": "* **Primary:** Matthews Correlation Coefficient (MCC). The paper argues that this is a more unbiased measure for imbalanced data than F1 or AUC.\n* **Additional:** Effect size (dominance) measured using Cliff's δ to quantify the practical significance of performance differences.",
    "question-9": "The paper does not mention using any specific dimensionality reduction techniques like PCA.",
    "question-10": "Yes, ensemble methods are a core part of the study:\n* **Traditional:** Bagging and Boosting are included as base learners.\n* **Imbalanced:** Many of the imbalanced techniques are based on bagging or boosting in combination with resampling methods.",
    "question-11": "Yes, 10 x 10-fold cross-validation was used. This means that each dataset was divided into 10 folds, and the process was repeated 10 times with different random fold assignments.",
    "question-12": "The ratio of defective to non-defective modules varied across the datasets (Table 8). The paper provides a detailed analysis of imbalance ratios in software defect prediction datasets, finding that most are not as severely imbalanced as in some other domains.",
    "question-13": "Attribute selection (feature selection) was performed on the *training* data for each base learner and within the imbalanced learning methods (see the pseudo-code in the \"Algorithm Evaluation\" section). The specific attribute selection method used is not specified.",
    "question-14": "Yes, attribute selection was performed, but the exact method is not specified.",
    "question-15": "Yes, the paper extensively reviews and discusses prior work on imbalanced learning for software defect prediction, highlighting inconsistencies and conflicting results from previous studies. The paper positions its work as a more comprehensive and systematic analysis.",
    "question-16": "WEKA (Waikato Environment for Knowledge Analysis), a popular machine learning toolkit in Java, was used.",
    "question-17": "The paper does not provide a detailed breakdown of strengths and weaknesses of each individual technique. However, the key findings are:\n    * The choice of both imbalanced learning method and base classifier has a significant impact.\n    * SVM (Support Vector Machines) consistently benefited from imbalanced learning techniques.\n    * Naïve Bayes was insensitive to imbalance.",
    "question-18": "The focus of the study is not explicitly stated, but based on the datasets and typical use of these datasets, it appears to be primarily on **within-project** defect prediction.",
    "question-19": "The paper does not propose novel techniques. It evaluates existing, established imbalanced learning methods from the machine learning literature.",
    "question-20": "* Blindly applying imbalanced learning is not a guaranteed solution.\n* The choice of both base classifier and imbalanced method is crucial. \n* SVM greatly benefits from imbalanced learning.\n* Naïve Bayes is generally insensitive to imbalance.\n* Strong imbalanced learners and classifiers sensitive to imbalance are preferred.\n* The type of input metric has less influence on the benefits of imbalanced learning.",
    "question-21": "* Data set characteristics beyond the imbalance ratio can influence performance (referring to work by López et al.).\n* Carefully selecting the right combination of imbalanced method and classifier is essential.",
    "question-22": "The paper does not analyze the computational complexity or runtime performance of the techniques.",
    "question-23": "* Explore other classifiers (e.g., neural networks, evolutionary algorithms).\n* Investigate defect data from non-open source projects.",
    "question-24": "Not explicitly discussed.",
    "question-25": "Yes, the paper uses:\n* Histograms to show data distributions (imbalance ratio, performance differences).\n* Boxplots to compare performance across different groups (classifiers, metrics, imbalanced methods).\n* Table 14:  A summary table using Win/Draw/Loss records to show statistically significant improvements of each imbalanced learner over each traditional learner, broken down by metric type.",
    "question-26": "The paper does not focus on model interpretability.",
    "question-27": "Not discussed, though the paper aims to provide actionable guidelines for practitioners.",
    "question-28": "Not analyzed.",
    "question-29": "Not explicitly compared. The paper focuses on three broad classes of metrics (code, network, and process) without delving into specific metric types within each class.",
    "question-30": "The authors acknowledge that further research using datasets from non-open source projects would strengthen the generalizability of their findings. They also encourage other researchers to replicate their study."
}