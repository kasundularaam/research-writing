{
"question-1": "2015",
"question-2": "The study used datasets from the PROMISE Software Engineering Repository. Specifically, they used projects CM1, JM1, KC1, KC2, and PC1.",
"question-3": "The combined dataset had 15,123 instances. Each of the 5 folds created for cross-validation had: \n\n* **Training:** 12,123 instances (with 2115 faulty - 17.45%)\n* **Testing:** 3,000 instances (with 550 faulty - 18.33%)",
"question-4": "* Adaptive Neuro Fuzzy Inference System (ANFIS)\n* Artificial Neural Network (ANN)\n* Support Vector Machine (SVM)",
"question-5": "Unfortunately, the paper focuses primarily on AUC-ROC and doesn't explicitly report accuracy, precision, recall, or F1-score.",
"question-6": "They used McCabe metrics:\n\n* `loc`: McCabe number of code lines\n* `v(g)`: Cyclomatic complexity\n* `ev(g)`: Essential complexity \n* `iv(g)`: Design complexity",
"question-7": "The paper doesn't provide individual predictive power or rankings for each metric. However, they analyze the importance of `ev(g)` and find it less influential than other metrics, leading to its exclusion in some experiments.",
"question-8": "The primary performance measure was the Area Under the ROC Curve (AUC-ROC).",
"question-9": "No, dimensionality reduction techniques like PCA weren't explicitly mentioned.",
"question-10": "No, ensemble methods weren't used in this study.",
"question-11": "Yes, they used **5-fold cross-validation**.",
"question-12": "* **Training data:** Approximately 17.45% faulty modules.\n* **Testing data:** Approximately 18.33% faulty modules.",
"question-13": "The paper doesn't explicitly mention specific data preprocessing techniques apart from creating the 5 folds for cross-validation.",
"question-14": "Yes, feature selection was performed based on an analysis of the influence of the `ev(g)` metric on fault prediction.  They removed `ev(g)` in later experiments as it seemed less influential.",
"question-15": "Yes, the authors compared their ANFIS results to previous studies that used ANN and SVM for software fault prediction. They referenced several studies from 2003 onwards, focusing on those that used similar datasets and evaluation criteria.",
"question-16": "The models were implemented using MATLAB 7.13.0 (R2011b).",
"question-17": "* **ANFIS:**  Strengths - combines expert knowledge and learning from data, potentially more robust to data changes; Weaknesses - requires more expert input during model design.\n* **ANN:**  Strengths - strong learning capability, good for nonlinear problems; Weaknesses - can be sensitive to data changes, may require careful parameter tuning.\n* **SVM:**  Strengths - effective for both linear and nonlinear data, can handle high-dimensional data; Weaknesses - training can be slow for large datasets, may be less interpretable than other methods.",
"question-18": "The study seems to be focused on **within-project defect prediction**, as they used datasets from specific projects within the PROMISE repository.",
"question-19": "The novel aspect of the study was the application of ANFIS to software fault prediction, which hadn't been done before in a comparative study with ANN and SVM.",
"question-20": "The authors concluded that ANFIS is a competitive method for software fault prediction, with performance comparable to ANN and better than SVM. They also suggested that using 3 McCabe metrics (excluding `ev(g)`) might be more effective than using all 4.",
"question-21": "They highlighted the challenge of data dependency in machine learning models for fault prediction. As projects evolve, changes in size, domain, and architecture can significantly affect the model's performance.",
"question-22": "No, there wasn't a detailed analysis of computational complexity or runtime performance.",
"question-23": "* They recommended further research with larger and diverse datasets to generalize their findings.\n* They also suggested investigating the use of domain expert knowledge in ANFIS modeling and exploring common architectural properties for improved performance.\n* They mentioned the need to develop metrics that take into account the use of external libraries and frameworks.",
"question-24": "No, transfer learning or domain adaptation wasn't discussed in the study.",
"question-25": "Yes, they provided ROC curves for visual comparison of the models' performance. They also included diagrams illustrating the structure of ANFIS and ANN.",
"question-26": "While they didn't explicitly discuss interpretability, they implied that ANFIS might offer some interpretability advantages due to the integration of expert knowledge.",
"question-27": "No specific industrial applications were mentioned.",
"question-28": "No cost-effectiveness analysis was provided.",
"question-29": "No, the focus was solely on McCabe metrics.",
"question-30": "They acknowledged that further research with diverse projects is needed to assess the generalizability of their findings."
}