{
"question-1": "2017",
"question-2": "Apache Rave (two releases: 0.22 and 0.23), Apache Commons Math (two releases: 2.2 and 3.0)",
"question-3": "Rave: 685 classes, Commons Math: 756 classes",
"question-4": "Na√Øve Bayes (NB), BayesNet (BN) with K2 search algorithm, Logitboost (LB), Adaboost (AB)",
"question-5": "NB: g-mean (Rave) 0.55, Accuracy (Rave) 0.59, g-mean (Math) 0.55, Accuracy (Math) 0.65, BN: g-mean (Rave) 0.58, Accuracy (Rave) 0.60, g-mean (Math) 0.58, Accuracy (Math) 0.67, LB: g-mean (Rave) 0.57, Accuracy (Rave) 0.60, g-mean (Math) 0.56, Accuracy (Math) 0.65, AB: g-mean (Rave) 0.57, Accuracy (Rave) 0.60, g-mean (Math) 0.57, Accuracy (Math) 0.66",
"question-6": "Coupling Between Objects (CBO), Number Of Children (NOC), Response For a Class (RFC), Depth of Inheritance Tree (DIT), Lack of Cohesion Among Methods (LCOM), Weighted Methods of a class (WMC), Source Lines of Code (SLOC)",
"question-7": "The paper doesn't explicitly report individual predictive power for each metric. However, it uses correlation analysis (Spearman's Rho) to analyze relationships between metrics and change proneness. It also uses Correlation-based Feature Selection (CFS) which identifies WMC, RFC, and SLOC for Rave, and RFC, LCOM, and SLOC for Math as the most important features. No explicit ranking or weighting is provided beyond this selection.",
"question-8": "g-mean, Accuracy",
"question-9": "No, but they used Correlation-based Feature Selection (CFS).",
"question-10": "Yes, both Logitboost and Adaboost are ensemble learning methods. Additionally, several of the search-based algorithms (like GFS-AB, GFS-LB) incorporate boosting within their framework.",
"question-11": "Yes, 10-fold cross-validation was used.",
"question-12": "Rave: 32.8% change-prone classes, 67.2% non-change-prone classes. Math: 23.54% change-prone classes, 76.46% non-change-prone classes.",
"question-13": "The paper doesn't explicitly mention any data preprocessing techniques besides the calculation of OO metrics and the mapping of change records to Java classes.",
"question-14": "Yes, Correlation-based Feature Selection (CFS) was used.",
"question-15": "Yes, the study compared the performance of search-based algorithms (SBAs) with traditional machine learning (ML) techniques. They also referenced several other studies in the related work section to position their research within the existing literature on change proneness prediction.",
"question-16": "Machine learning models: WEKA (open-source tool in Java), Search-based algorithms: KEEL (open-source tool in Java)",
"question-17": "The paper doesn't provide a detailed analysis of the strengths and weaknesses of each individual ML technique. The focus is more on the overall comparison between SBAs and ML.",
"question-18": "The study focused on within-project change proneness prediction. They analyzed consecutive releases of the same software projects.",
"question-19": "The study itself doesn't propose new techniques. However, it evaluates the application of existing hybrid search-based algorithms that combine SBAs with other techniques (e.g., GFS-AB combines fuzzy logic with Adaboost, PSOLDA combines Particle Swarm Optimization with Linear Discriminant Analysis).",
"question-20": "The study concluded that search-based algorithms (specifically PSOLDA) outperformed traditional machine learning techniques in predicting change-prone classes for the studied datasets. However, this difference was not statistically significant.",
"question-21": "Imbalanced datasets: The authors acknowledge that the datasets used were imbalanced, which can pose a challenge for accurate prediction. Computational time: They noted that some SBAs can be computationally expensive, especially for larger projects.",
"question-22": "Yes, the study reported the computational time taken by each technique and discussed the trade-off between performance and CPU time.",
"question-23": "Replicate the study with more datasets to improve generalizability. Investigate the confounding effect of size on the relationship between metrics and change proneness. Explore the use of parallel or cloud-based approaches to address computational time concerns with SBAs.",
"question-24": "No, transfer learning or domain adaptation was not considered in this study.",
"question-25": "Yes, the paper includes bar charts and scatter plots to compare the performance of SBAs and ML techniques.",
"question-26": "No, the study doesn't explicitly discuss the interpretability of the models.",
"question-27": "The paper discusses the potential applications of the findings for software practitioners in terms of resource allocation, design decisions, and testing efforts.",
"question-28": "No, cost-effectiveness was not explicitly analyzed.",
"question-29": "No, the study focused on traditional OO metrics.",
"question-30": "The authors acknowledge that the generalizability of their findings might be limited due to using only two open-source projects. They recommend further research with diverse datasets to improve generalizability."
}