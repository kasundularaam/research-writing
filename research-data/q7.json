{
"question-1": "2014",
"question-2": "[\"Ant-1.7\", \"Camel-1.6\", \"KC3\", \"MC1\", \"PC2\", \"PC4\"]",
"question-3": "{\n    \"Ant-1.7\": 745,\n    \"Camel-1.6\": 965,\n    \"KC3\": 200,\n    \"MC1\": 1988,\n    \"PC2\": 1585,\n    \"PC4\": 1287\n}",
"question-4": "[\"Average Probability Ensemble (APE)\", \"Weighted Support Vector Machines (W-SVMs)\", \"Random Forests\"]",
"question-5": "The paper primarily focuses on AUC-ROC and G-Mean due to imbalanced datasets. Individual precision, recall, and F1-score are not reported. Refer to Table 3, Table 5, and Table 6 in the paper for specific AUC and G-Mean values.",
"question-6": "51 software metrics were used. Refer to Table 2 in the paper for the complete list. Examples include:\n- Decision count (M_1)\n- Cyclomatic complexity (M_2)\n- Global data density (M_3)\n- Halstead difficulty (M_4)",
"question-7": "Individual metric predictive power and ranking are not explicitly provided. The paper focuses on feature selection methods for selecting relevant subsets.",
"question-8": "[\"AUC-ROC\", \"G-mean\"]",
"question-9": "Dimensionality reduction techniques were not used explicitly, but feature selection served as a form of dimensionality reduction.",
"question-10": "Yes, the proposed Average Probability Ensemble (APE) is an ensemble method, combining multiple base learners: Random Forests, Gradient Boosting, Stochastic Gradient Descent, Weighted Support Vector Machines, Logistic Regression, Multinomial Naive Bayes, Bernoulli Naive Bayes.",
"question-11": "Yes, stratified 10-fold cross-validation was used.",
"question-12": "Defect ratio varied across datasets, ranging from 1.01 to 22.28 (defective to total samples).",
"question-13": "Stratified sampling was used to maintain class distribution during cross-validation.",
"question-14": "Yes, feature selection methods evaluated include:\n- Pearson's correlation\n- Fisher's criterion\n- Greedy Forward Selection (GFS) - performed best",
"question-15": "The APE was compared against:\n- W-SVMs\n- Random Forests\nFeature selection methods were also compared.",
"question-16": "Python",
"question-17": "{\n\"APE\": {\n\"strengths\": \"Robustness to imbalanced data and redundant features.\",\n\"weaknesses\": \"Not provided\"\n},\n\"W-SVMs\": {\n\"strengths\": \"Designed to handle imbalanced data.\",\n\"weaknesses\": \"Performance can degrade significantly with poorly selected features.\"\n},\n\"Random Forests\": {\n\"strengths\": \"Often performs well, robust.\",\n\"weaknesses\": \"Performance might not be as good as carefully designed ensembles.\"\n}\n}",
"question-18": "The paper doesn't explicitly state whether it's within-project or cross-project, but it uses standard defect datasets, which are often used for within-project prediction.",
"question-19": "Yes, the Average Probability Ensemble (APE) is a novel ensemble method. Enhanced APE (APE with GFS) is a hybrid approach.",
"question-20": "Ensemble learning, specifically APE, outperforms single classifiers (W-SVMs, RF). Feature selection is crucial for improving performance. GFS is an effective feature selection method.",
"question-21": "Data imbalance and the quality of software defect datasets are key challenges and limitations.",
"question-22": "No, computational complexity analysis is not provided.",
"question-23": "Future research includes applying other feature selection techniques and exploring other ensemble learners.",
"question-24": "No, transfer learning or domain adaptation is not discussed.",
"question-25": "Yes, several figures are included: ROC curves, scatter plots for decision boundaries.",
"question-26": "Interpretability is not explicitly discussed.",
"question-27": "Industrial applications are not discussed.",
"question-28": "Cost-effectiveness is not analyzed.",
"question-29": "Comparison with advanced metrics is not performed.",
"question-30": "The authors acknowledge that the selected datasets may not be representative of all software projects, but a more detailed discussion on generalizability is not provided."
}