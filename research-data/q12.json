{
"question-1": "2019",
"question-2": "AR1, AR4, AR6, CM1, KC1, KC2, MW1, PC1, PC3, and PC4. These are all from the NASA MDP repository.",
"question-3": {
"AR1": 114,
"AR4": 94,
"AR6": 87,
"CM1": 446,
"KC1": 2032,
"KC2": 513,
"MW1": 366,
"PC1": 1024,
"PC3": 1475,
"PC4": 1380
},
"question-4": "SDNN (Siamese Dense Neural Network), SDNNÂ¯ (SDNN without cosine-proximity in the metering function), DNN (Deep Neural Network), LSTM (Long Short-Term Memory network), DBN (Deep Belief Network), LR (Logistic Regression), BAG (Bagging), NB (Naive Bayes), TNB (Transfer Naive Bayes), DTB (Double Transfer Boosting)",
"question-5": "This information is NOT directly provided for all the methods. The paper focuses on comparing:\n\n* **PD (Probability of Detection):** This is equivalent to Recall.\n* **PF (Probability of False):** Related to Specificity, not Precision.\n* **F-measure:** This combines Precision and Recall.\n* **MCC (Matthews Correlation Coefficient):** A balanced measure. \n* **AUC (Area Under the ROC Curve):** Used for stability analysis.\n\nThe exact values for Precision and Recall (except PD) are not provided. You can find the reported results in Tables 3, 4, 7, and in Appendix A.",
"question-6": "The paper states that 21 commonly used method-level metrics are used, but the specific metrics are not listed. It refers to [3] for more details.",
"question-7": "This information is NOT provided in the paper.",
"question-8": "PD (Probability of Detection), PF (Probability of False), F-measure, MCC (Matthews Correlation Coefficient), AUC (Area Under the ROC Curve)",
"question-9": "No, dimensionality reduction techniques are not mentioned.",
"question-10": "Yes, the following ensemble methods were included in the comparisons:\n\n* BAG (Bagging)\n* DTB (Double Transfer Boosting)",
"question-11": "The paper mentions that each dataset was divided into ten sub-datasets (with the same imbalance rate of 10%). This could be interpreted as a form of 10-fold cross-validation, but it's not explicitly stated.",
"question-12": "This information is provided in Table 1 under the \"Imbalanced rate\" column. It represents the rounded-down ratio of non-defects to defects.",
"question-13": "Yes, the following data preprocessing techniques were used:\n\n* Deletion of repeated entities\n* Replacement of missing values using the average of the corresponding metric\n* Data normalization using min-max normalization\n* Data oversampling using SMOTETomek to address class imbalance ",
"question-14": "No, feature selection is not mentioned in the paper.",
"question-15": "Yes, the proposed SDNN method was compared against several other methods, including:\n\n* DNN, LSTM, DBN, LR, BAG, NB, TNB, and DTB (all listed in answer 4).\n* Comparisons are also made to results in other publications (references [45], [47], [51]).",
"question-16": "The paper mentions using \"tensorflow, keras and matlab environments.\"",
"question-17": "Strengths and weaknesses are discussed to some extent:\n\n* **SDNN:** Shown to be effective with small data, benefits from the Siamese architecture and the use of cosine-proximity. Training time is relatively long. \n* **Other deep learning methods (DNN, LSTM, DBN):** Less effective with small data, sensitive to class imbalance. \n* **TNB:** Good PD but high PF, making it less practically useful.\n* **Classical methods (NB, LR, BAG):** Generally outperformed by SDNN.",
"question-18": "The paper does not explicitly state this. The datasets are from different software projects, but how the training and testing were split (within-project or cross-project) is not clear.",
"question-19": "Yes, the main novelty is the proposed **SDNN (Siamese Dense Neural Network)** architecture, specifically designed for software defect prediction with small datasets. The key elements are:\n\n* **Siamese structure:** Two identical sub-networks process input pairs.\n* **Dense sub-networks:** Using fully-connected layers to learn features.\n* **Metering function:** Combining Euclidean distance and cosine-proximity to measure similarity between feature vectors.\n* **Contrastive loss function:** Optimizing the network for learning similar and dissimilar feature representations.",
"question-20": "The main conclusion is that **SDNN is a competitive approach for software defect prediction, particularly when data is limited**. It outperformed other benchmarked methods in terms of PD, PF, F-measure, MCC, and showed more stable performance under varying class imbalance conditions.",
"question-21": "* **Limited data:** The paper focuses on this challenge as a primary motivation.\n* **Class imbalance:** Common in defect datasets and addressed using the SMOTETomek technique.\n* **Lack of consensus on performance metrics:** This is mentioned as a threat to construct validity.",
"question-22": "Yes, Table 6 presents the training and testing time for the different methods on each dataset. SDNN had the second-longest average training time, but the authors consider the testing time acceptable.",
"question-23": "* Conduct further empirical studies on more defect datasets.\n* Extend the method to multi-category defect prediction.\n* Explore techniques to speed up the training process, like parallel computing.",
"question-24": "Transfer learning is implicitly involved in the **TNB (Transfer Naive Bayes)** method, which is included in the comparisons. However, the paper's focus is not on evaluating or improving transfer learning methods.",
"question-25": "Yes, the paper includes:\n\n* **Scatter plots:** Showing PD vs. PF for different methods on various datasets (Figures 4, 6, 7).\n* **Line plots:** Illustrating AUC values against increasing class imbalance rates (Figure 5).",
"question-26": "No, the paper does not discuss model interpretability.",
"question-27": "The paper does not mention specific industrial applications. It primarily focuses on evaluating the effectiveness of the proposed method on publicly available datasets.",
"question-28": "No, cost-effectiveness is not analyzed in the paper.",
"question-29": "No, the paper does not delve into comparing different types of software metrics. It uses a set of 21 commonly used method-level metrics without further discussion on their selection or individual relevance.",
"question-30": "This is briefly addressed as a potential threat to external validity. The authors acknowledge that further research on more diverse datasets is needed to strengthen the generalizability of their findings."
}