{
"question-1": "2018",
"question-2": "30 open-source Java projects from PROMISE and NASA data repository. The specific project names are listed in Table 4.",
"question-3": "The number of instances (classes) in each dataset varies. See Table 4 for the specific numbers.",
"question-4": "Least Squares Support Vector Machine (LSSVM) with three different kernel functions:\n   * Linear kernel\n   * Polynomial kernel\n   * Radial Basis Function (RBF) kernel",
"question-5": "   * **Accuracy and F-Measure:** Values for each dataset, feature selection method, and kernel function are reported in Table 9a-c.\n   * **Precision, Recall, AUC-ROC:** These values are not explicitly reported in the paper.",
"question-6": "20 Object-Oriented metrics are used, including:\n   * Weighted Methods per Class (WMC)\n   * Depth of Inheritance Tree (DIT)\n   * Number of Children (NOC)\n   * Coupling Between Objects (CBO)\n   * Response for a Class (RFC)\n   * Lack of Cohesion in Methods (LCOM)\n   * Afferent Coupling (Ca)\n   * Efferent Coupling (Ce)\n   * Number of Public Methods (NPM)\n   * LCOM3\n   * Lines of Code (LOC)\n   * Data Access Metric (DAM)\n   * Measure of Aggregation (MOA)\n   * Measure of Functional Abstraction (MFA)\n   * Cohesion Among Methods of Class (CAM)\n   * Inheritance Coupling (IC)\n   * Coupling Between Methods (CBM)\n   * Average Method Complexity (AMC)\n   * Maximum McCabe's Cyclomatic Complexity (Max-CC)\n   * Average McCabe's Cyclomatic Complexity (Avg-CC)",
"question-7": "   * The paper does not report individual predictive power for each metric.\n   * Metrics are ranked and selected using feature selection techniques. See questions 9 and 14 for details.",
"question-8": "   * Accuracy\n   * F-Measure\n   * Normalized Estimated Fault Removal Cost (NEcost)",
"question-9": "Principal Component Analysis (PCA) is used as a feature ranking method.",
"question-10": "No ensemble methods were used in this study.",
"question-11": "20-fold cross-validation was used to evaluate and compare the models.",
"question-12": "The percentage of defective (faulty) classes varies for each dataset. See Table 4 for the specific percentages.",
"question-13": "   * Data is normalized using Min-Max normalization to the range [0,1].",
"question-14": "Ten different feature selection techniques are used:\n    * **Feature Ranking:**\n        * Chi Squared test\n        * Gain Ratio Feature Evaluation\n        * Information Gain Feature Evaluation\n        * OneR Feature Evaluation\n        * Logistic Regression Analysis\n        * Principal Component Analysis (PCA)\n    * **Feature Subset Selection:**\n        * Correlation Based Feature Selection (CFS)\n        * Classifier Subset Evaluation\n        * Filtered Subset Evaluation\n        * Rough Set Analysis (RSA)",
"question-15": "   * The study compares the performance of LSSVM with different kernels.\n   * It also compares the performance of LSSVM against other commonly used classification techniques (Logistic Regression, Decision Tree, Naive Bayes, Neural Network, SVM with various kernels).",
"question-16": "The machine learning models were implemented using MATLAB.",
"question-17": "    * The paper primarily focuses on comparing the overall performance of different techniques and does not explicitly discuss individual strengths and weaknesses.\n    * However, it highlights that LSSVM with the RBF kernel performs well in general.",
"question-18": "The study focuses on within-project defect prediction, aiming to identify faulty classes within a specific software project.",
"question-19": "No novel or hybrid techniques were explicitly proposed in this study. The focus is on evaluating existing techniques using a cost-based evaluation framework.",
"question-20": "   * LSSVM with the RBF kernel, coupled with feature selection using Rough Set Analysis or PCA, was found to be effective.\n   * Feature selection techniques were generally found to improve prediction accuracy compared to using all metrics.",
"question-21": "   * The paper highlights that psychological factors and team-related aspects are not considered, which can impact real-world software reliability.\n   * It also mentions the limitation of only predicting whether a class is faulty or not, without providing information on the possible number of bugs within the class.",
"question-22": "The paper does not explicitly discuss the computational complexity or runtime performance of the techniques used.",
"question-23": "   * Extending the work to predict the number of bugs within a faulty class.\n   * Replicating the study on other programming paradigms and languages.\n   * Incorporating psychological factors and team-related aspects into the models.",
"question-24": "This aspect was not considered in the study.",
"question-25": "   * Box-plot diagrams are used to visualize the distribution of Accuracy and F-Measure for different feature selection techniques (Fig. 8a-c).\n   * Similar box-plot diagrams are used for comparing the performance of LSSVM with other classifiers (Fig. 11).\n   * Figures are used to depict the NEcost of fault prediction techniques for varying fault identification efficiencies (Figs. 13-15).\n   * Heatmaps are used to visualize the results of t-test analysis (Figs. 9 and 12).",
"question-26": "The paper does not specifically discuss the interpretability of the models used.",
"question-27": "While the study uses open-source datasets, it discusses the practical implications of the findings for real-world software development, particularly regarding the use of feature selection to reduce complexity and improve model efficiency.",
"question-28": "The paper proposes and uses a cost-based evaluation framework to assess the cost-effectiveness of the fault prediction approach, considering fault removal costs at different testing phases.",
"question-29": "The study focuses on traditional object-oriented software metrics and does not compare them with more advanced or domain-specific metrics.",
"question-30": "The authors acknowledge that the study's external validity might be limited due to using only Java projects. They suggest further research to replicate the findings on other programming languages and paradigms. However, they believe that the general principles and insights gained from the study can be applied to other object-oriented software development projects."
}